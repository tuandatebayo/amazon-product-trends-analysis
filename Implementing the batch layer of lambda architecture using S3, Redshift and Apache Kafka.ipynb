{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the batch layer of lambda architecture using S3, Redshift and Apache Kafka\n",
    "\n",
    "### Purpose:\n",
    "- store all the tweets that were produced by Kafka Producer into S3\n",
    "- export them into Redshift\n",
    "- perform aggregation on the tweets to get the desired output of batch layer\n",
    "- achieve this by: \n",
    "    - every couple of hours get the latest unseen tweets produced by the Kafka Producer and store them into a S3 archive\n",
    "    - every night run a sql query to compute the result of batch layer\n",
    "\n",
    "### Contents: \n",
    "- [Defining the Kafka consumer](#1)\n",
    "- [Defining a Amazon Web Services S3 storage client](#2)\n",
    "- [Writing the data into a S3 bucket](#3)\n",
    "- [Exporting data from S3 bucket to Amazon Redshift using COPY command](#4)\n",
    "- [Aggregating \"raw\" tweets in Redshift](#5)\n",
    "- [Deployment](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from io import StringIO\n",
    "import boto3\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "### Defining the Kafka consumer\n",
    "- setting the location of Kafka Broker\n",
    "- specifying the group_id and consumer_timeout\n",
    "- subsribing to a topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "                        bootstrap_servers='localhost:9092',\n",
    "                        auto_offset_reset='latest',  # Reset partition offsets upon OffsetOutOfRangeError\n",
    "                        group_id='test',   # must have a unique consumer group id \n",
    "                        consumer_timeout_ms=1000)  \n",
    "                                # How long to listen for messages - we do it for 10 seconds \n",
    "                                # because we poll the kafka broker only each couple of hours\n",
    "\n",
    "consumer.subscribe('test-amazon-crawler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for message in consumer:\n",
    "#     print(message.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "### Defining a Amazon Web Services S3 storage client\n",
    "- setting the autohrizaition and bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='ap-southeast-2',\n",
    "    aws_access_key_id='AKIAQE3ROLEGP2T7SKWS',\n",
    "    aws_secret_access_key='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',\n",
    ")\n",
    "\n",
    "s3_client = s3_resource.meta.client\n",
    "bucket_name = 'bigdata20241-14'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigdata20241-14\n"
     ]
    }
   ],
   "source": [
    "#print out bucket names\n",
    "for bucket in s3_resource.buckets.all():\n",
    "    print(bucket.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "### Writing the data into a S3 bucket\n",
    "- polling the Kafka Broker\n",
    "- aggregating the latest messages into a single object in the bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data(path):\n",
    "    csv_buffer = StringIO() # S3 storage is object storage -> our document is just a large string\n",
    "\n",
    "    for message in consumer: # this acts as \"get me an iterator over the latest messages I haven't seen\"\n",
    "        csv_buffer.write(message.value.decode() + '\\n') \n",
    "        print(message)\n",
    "    s3_resource.Object(bucket_name,path).put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_buffer = StringIO()\n",
    "# for message in consumer: # this acts as \"get me an iterator over the latest messages I haven't seen\"\n",
    "#         csv_buffer.write(message.value.decode() + '\\n') \n",
    "#         print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test/'+ time.strftime(\"%Y/%m/%d/%H\") + '_test_' + str(random.randint(1,1000)) + '.log'\n",
    "\n",
    "store_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "### Exporting data from S3 bucket to Amazon Redshift using COPY command\n",
    "- authenticate and create a connection using psycopg module\n",
    "- export data using COPY command from S3 to Redshift \"raw\" table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import psycopg2\n",
    "\n",
    "def get_redshift_config():\n",
    "    # Create a Secrets Manager client\n",
    "    client = boto3.client('secretsmanager', region_name='us-east-1')\n",
    "    \n",
    "    try:\n",
    "        # Replace 'redshift/credentials' with your secret name\n",
    "        get_secret_value_response = client.get_secret_value(SecretId='redshift/credentials')\n",
    "        \n",
    "        # Parse the secret value\n",
    "        secret = json.loads(get_secret_value_response['SecretString'])\n",
    "        \n",
    "        # Extract database connection parameters\n",
    "        config = {\n",
    "            'dbname': secret['dbname'],\n",
    "            'user': secret['username'],\n",
    "            'password': secret['password'],\n",
    "            'host': secret['host'],\n",
    "            'port': secret['port']\n",
    "        }\n",
    "        return config\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error retrieving secrets:\", e)\n",
    "        return None\n",
    "\n",
    "# Connect to Redshift using psycopg2\n",
    "config = get_redshift_config()\n",
    "if config:\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            dbname=config['dbname'],\n",
    "            user=config['user'],\n",
    "            password=config['password'],\n",
    "            host=config['host'],\n",
    "            port=config['port']\n",
    "        )\n",
    "        print(\"Connection successful\")\n",
    "        # Perform your database operations here\n",
    "        # ...\n",
    "        \n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error connecting to the database:\", e)\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "config = { 'dbname': 'dev', \n",
    "           'user':'awsuser',\n",
    "           'pwd':'ZKEMKhrkik375*)',\n",
    "           'host':'redshift-cluster-2.cp7qgj34ssr1.ap-southeast-2.redshift.amazonaws.com',\n",
    "           'port':'5439'\n",
    "         }\n",
    "\n",
    "conn =  psycopg2.connect(dbname=config['dbname'], host=config['host'], \n",
    "                              port=config['port'], user=config['user'],\n",
    "                              password=config['pwd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files(conn, path):\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"\"\" \n",
    "        copy \n",
    "            batch_raw\n",
    "        from \n",
    "            's3://bigdata20241-14/\"\"\" + path + \"\"\"'  \n",
    "            access_key_id 'AKIAQE3ROLEGP2T7SKWS'\n",
    "            secret_access_key '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'\n",
    "            delimiter ';'\n",
    "            region 'ap-southeast-2'\n",
    "    \"\"\")\n",
    "    curs.close()\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the batch layer output\n",
    "- querying the raw tweets stored in redshift to get the desired batch layer output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_batch_layer(conn):\n",
    "    curs = conn.cursor()\n",
    "    curs.execute(\"\"\" \n",
    "        drop table if exists batch_layer;\n",
    "\n",
    "        with raw_dedup as (\n",
    "        SELECT\n",
    "            distinct id,created_at,followers_count,location,favorite_count,retweet_count\n",
    "        FROM\n",
    "            batch_raw\n",
    "        ),\n",
    "        batch_result as (\n",
    "            SELECT\n",
    "                location,\n",
    "                count(id) as count_id,\n",
    "                sum(followers_count) as sum_followers_count,\n",
    "                sum(favorite_count) as sum_favorite_count,\n",
    "                sum(retweet_count) as sum_retweet_count\n",
    "            FROM\n",
    "                raw_dedup\n",
    "            group by \n",
    "                location\n",
    "        )\n",
    "        select \n",
    "            *\n",
    "        INTO\n",
    "            batch_layer\n",
    "        FROM\n",
    "            batch_result\"\"\")\n",
    "    curs.close()\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedTable",
     "evalue": "relation \"batch_raw\" does not exist\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcompute_batch_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mcompute_batch_layer\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_batch_layer\u001b[39m(conn):\n\u001b[1;32m      2\u001b[0m     curs \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mcurs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m        drop table if exists batch_layer;\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m        with raw_dedup as (\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m        SELECT\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m            distinct id,created_at,followers_count,location,favorite_count,retweet_count\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m        FROM\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m            batch_raw\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m        ),\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m        batch_result as (\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m            SELECT\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m                location,\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m                count(id) as count_id,\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m                sum(followers_count) as sum_followers_count,\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m                sum(favorite_count) as sum_favorite_count,\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m                sum(retweet_count) as sum_retweet_count\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m            FROM\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m                raw_dedup\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43m            group by \u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43m                location\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43m        )\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m        select \u001b[39;49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;43m            *\u001b[39;49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;43m        INTO\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;43m            batch_layer\u001b[39;49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;43m        FROM\u001b[39;49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;43m            batch_result\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     curs\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     31\u001b[0m     conn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[0;31mUndefinedTable\u001b[0m: relation \"batch_raw\" does not exist\n"
     ]
    }
   ],
   "source": [
    "compute_batch_layer(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "### Deployment \n",
    "- perform the task every couple of hours and wait in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_work(interval):\n",
    "    while True:\n",
    "        path = 'tweets/'+ time.strftime(\"%Y/%m/%d/%H\") + '_tweets_' + str(random.randint(1,1000)) + '.log'\n",
    "        store_twitter_data(path)\n",
    "        copy_files(conn, path)\n",
    "        #interval should be an integer, the number of seconds to wait\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodic_work(60 * 60) ## 60 minutes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'tweets/'+ time.strftime(\"%Y/%m/%d/%H\") + '_tweets_' + str(random.randint(1,1000)) + '.log'\n",
    "\n",
    "store_twitter_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 4))\n",
      "\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "SSL SYSCALL error: Operation timed out\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-283833030d14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcopy_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-e3e962d1c071>\u001b[0m in \u001b[0;36mcopy_files\u001b[0;34m(conn, path)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdelimiter\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mregion\u001b[0m \u001b[0;34m'eu-central-1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \"\"\")\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: SSL SYSCALL error: Operation timed out\n"
     ]
    }
   ],
   "source": [
    "copy_files(conn, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run at the end of the day\n",
    "compute_batch_layer(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
